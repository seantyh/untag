{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235ced54-5243-440d-9a61-8559081b827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d001407-e797-45a0-bb39-38abf8b5a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c46a5a-0060-4e1f-a7a3-e9840313ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler)\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "from transformers import set_seed, get_scheduler\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import MultipleChoiceModelOutput\n",
    "from transformers import AutoModelForMultipleChoice, BertTokenizerFast\n",
    "import datasets\n",
    "from datasets import Dataset, load_metric\n",
    "from PromptTuningBERT_allchoice_v2 import BertPromptForMultipleChoice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276c7c8-d310-4f5a-897a-62d048bc91f6",
   "metadata": {},
   "source": [
    "# Setting up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989b187",
   "metadata": {},
   "source": [
    "## Loading Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be03755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server Paths \n",
    "# ..data/avo727/PromptTuning/CWNdata/Sean_PT2_encoded_dataset\n",
    "maindir = \"/mnt/md0/data/avo727/PromptTuning\"\n",
    "datadir = f\"{maindir}/CWN_data\"\n",
    "preddir = f\"{maindir}/model_predictions\"\n",
    "datasetdir= f\"{maindir}/CWNdata/PT2_allchoice_encoded_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1fc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(datasetdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6317182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset['test']['numchoices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f595b9",
   "metadata": {},
   "source": [
    "## Data Collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e38121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5314ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    allchoice version: ÂÖ®Êï∏Êî§Âπ≥ÈÄÅÂæÄmodelÂÖßÈÉ®ËôïÁêÜÔºà‰∏çÁ∂ìunflattenÔºåÁ¢∫‰øùÂÇ≥Ëº∏numchoicesÔºâ\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizer\n",
    "    padding, trunc = True, True\n",
    "    max_length =  None\n",
    "    pad_to_multiple_of = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        pin_label = True if \"label\" in features[0].keys() else False\n",
    "        accepted_keys = [\"input_ids\", \"attention_mask\", \"label\", \n",
    "                         \"token_type_ids\", 'class_selector', 'numchoices']\n",
    "        batch_size = len(features)\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        numchoices = [feature.pop('numchoices') for feature in features]\n",
    "        seq_classes = [feature.pop('class_selector') for feature in features]\n",
    "        \n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items() if k in accepted_keys} \n",
    "                               for i in range(nc)] for feature, nc in zip(features, numchoices)]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding= \"longest\",\n",
    "            max_length= self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # processing labels: \n",
    "        proclabels = []\n",
    "        for l, nc in zip(labels, numchoices):\n",
    "            label = [0]*nc\n",
    "            label[l] = 1\n",
    "            proclabels.extend(label)\n",
    "        batch = {k: v for k, v in batch.items() if k in accepted_keys}\n",
    "        assert len(proclabels) ==  len(batch['input_ids'])\n",
    "        # print(len(batch['input_ids'])) # print(sum([len(x) for x in seq_classes])) # should match \n",
    "        # flattening list of lists\n",
    "        batch[\"class_selector\"] = torch.tensor(sum(seq_classes, [])) \n",
    "        batch[\"labels\"] = torch.tensor(proclabels, dtype=torch.int64)\n",
    "        batch[\"numchoices\"] = torch.tensor(numchoices, dtype=torch.int64)\n",
    "        # all flattened into instances (instead of problems)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5542549",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b99676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( predictions: typing.Union[numpy.ndarray, typing.Tuple[numpy.ndarray]]\n",
    "# label_ids: typing.Union[numpy.ndarray, typing.Tuple[numpy.ndarray]] )\n",
    "class ComputeMetrics:\n",
    "    from datasets import Dataset, load_metric\n",
    "    datasetdir= f\"{maindir}/CWNdata/PT2_allchoice_encoded_dataset\"\n",
    "    encoded_dataset = datasets.load_from_disk(datasetdir)\n",
    "    numchoices = encoded_dataset['test']['numchoices']\n",
    "    def __call__(self, logits, labels):\n",
    "        # eval_batch_size = len(encoded_dataset['test'])\n",
    "        # È°åËôüÔºålabels(ÈÇÑÂéü)ÔºåÈ†êÊ∏¨label \n",
    "        from collections import defaultdict \n",
    "        mapping, label_mapping, prediction_mapping = defaultdict(list),defaultdict(list), defaultdict(list)\n",
    "        curstart = 0\n",
    "        accu = defaultdict(int)\n",
    "        accdict = {}\n",
    "        for exid, nc in enumerate(self.numchoices):\n",
    "            label_id = labels[exid]\n",
    "            pred_id = np.argmax([logits[i] for i in range(curstart, curstart+nc)])\n",
    "            mapping[nc].append(exid) \n",
    "            label_mapping[nc].append(label_id)\n",
    "            prediction_mapping[nc].append(pred_id)\n",
    "            if label_id == pred_id: \n",
    "                accu[nc] += 1\n",
    "            # update \n",
    "            curstart = curstart+nc\n",
    "        cnt_numchoices = len(mapping) # 2\n",
    "        # print(f'* There are {cnt_numchoices} numchoices in the dataset.')\n",
    "        # return {'accuracy': (preds == label_ids).astype(np.float32).mean().item()}\n",
    "        for nc, nctotal in mapping.items():\n",
    "            accurates = accu[nc]\n",
    "            accdict[nc] = (accurates/len(nctotal))\n",
    "        accurates = sum(accu.values())\n",
    "        return accurates/len(self.numchoices), accdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d68e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SchedulerType(ExplicitEnum):\n",
    "#     LINEAR = \"linear\"\n",
    "#     COSINE = \"cosine\"\n",
    "#     COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "#     POLYNOMIAL = \"polynomial\"\n",
    "#     CONSTANT = \"constant\"\n",
    "#     CONSTANT_WITH_WARMUP = \"constant_with_warmup\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f47cc-fe1d-4ebe-a17b-60a61007ec65",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c6643-abd5-4f46-bbb2-e86496cc7449",
   "metadata": {},
   "source": [
    "## üé≤ Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ba895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparams\n",
    "base_model = 'bert-base-chinese'\n",
    "batchsize = 4\n",
    "prompt_len = n_tokens = 12\n",
    "lr = 5e-4 # 5e-4\n",
    "scheduler_type = \"linear\"\n",
    "wd = 0.005 # best 0.005\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 1 # pytorch trainer default \n",
    "myseed = 1126   # best 1126\n",
    "epochs = 20     # best 20\n",
    "# accumulation_steps = 1 # pytorch trainer default \n",
    "###### task-specific stats ##### \n",
    "nclass = 19\n",
    "TESTSIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6282b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PromptTuningBERT_allchoice_v2 import BertPromptForMultipleChoice\n",
    "config = {\n",
    "    'n_tokens': prompt_len ,\n",
    "    'n_class': nclass,\n",
    "    'train_bert': False,\n",
    "    'to_debug': False,\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7444fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''seeding'''\n",
    "\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "# set_seeds(myseed)\n",
    "set_seed(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1947e222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scheduler with warm up'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''grad accum steps'''\n",
    "# https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3\n",
    "'''trainer'''\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L1453\n",
    "'''scheduler with warm up'''\n",
    "# https://github.com/huggingface/transformers/blob/198c335d219a5eb4d3f124fdd1ce1a9cd9f78a9b/src/transformers/optimization.py#L75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2647a4-37e5-4707-86d3-000358e45a4d",
   "metadata": {},
   "source": [
    "```\n",
    "import wandb\n",
    "wandb.init(project='prompt_tuning_rp_v3', name = runname, \n",
    "           entity='nana2929', group=\"no trainer\")\n",
    "wandb.config.update({\n",
    "    'learning_rate':lr,\n",
    "    'batch_size':batchsize,\n",
    "    'weight_decay':wd,\n",
    "    'seed':myseed,\n",
    "    'n_tokens':n_tokens, \n",
    "    'epochs':epochs, \n",
    "    'scheduler': scheduler_type,  \n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d9859",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üê£ Dummy Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a563c334-37ac-4c53-8e25-d36d1516a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare data loader\n",
    "#\n",
    "train_loader = DataLoader(encoded_dataset['train'], \n",
    "                    shuffle=True, \n",
    "                    collate_fn=DataCollatorForMultipleChoice(), \n",
    "                    batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6e47c-10b9-4e70-89bc-91a33a12ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertPromptForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# toy example \n",
    "model = BertPromptForMultipleChoice.from_pretrained(base_model, config)\n",
    "model = model.to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                        lr=3e-4, \n",
    "                        weight_decay = wd, # ÊúâÂÖ©Á®Æweight decayÊñπÊ≥ï\n",
    "                        eps = 1e-08) # 1e-08, training args default \n",
    "loss_mean = MeanMetric()\n",
    "loss_vec = []\n",
    "# use when debugging\n",
    "# from itertools import islice\n",
    "# train_batches = list(islice(train_loader, 0, 2))\n",
    "train_batches = train_loader\n",
    "\n",
    "for i in tqdm(range(16)):   # epochs = 10 \n",
    "    for batch in train_batches:\n",
    "        model.train()\n",
    "        batch = {k: batch[k].to('cuda') for k in batch}\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_mean.update(loss.item())        \n",
    "    loss_vec.append(loss_mean.compute())\n",
    "    loss_mean.reset()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137172ec-f0da-460b-9a04-4ef4f6e850f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_vec) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542f6b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üçÄ Manual Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8658a38-4a8d-486c-8ccd-40c21fc10411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Tuning hypers\n",
    "# --------------\n",
    "batchsize = 4\n",
    "epochs = 16\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c527ec4-80c6-43a3-b5d0-38f96ac16953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare data loader\n",
    "#\n",
    "\n",
    "train_loader = DataLoader(encoded_dataset['train'], \n",
    "                    shuffle=True, \n",
    "                    collate_fn=DataCollatorForMultipleChoice(), \n",
    "                    batch_size=batchsize)\n",
    "eval_loader = DataLoader(encoded_dataset['test'], \n",
    "                    shuffle=False, \n",
    "                    collate_fn=DataCollatorForMultipleChoice(), \n",
    "                    batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08581bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\tName of the run: 0330-1323_RPBert_allc\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# creating model directory\n",
    "#\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.now()\n",
    "timeprefix = now.strftime(\"%m%d-%H%M\")\n",
    "runname = f'{timeprefix}_RPBert_allc'\n",
    "\n",
    "model_dir = f'../data/runs_v2/{runname}'\n",
    "print('*\\tName of the run:', runname)\n",
    "os.makedirs(model_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a9b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c555199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertPromptForMultipleChoice: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertPromptForMultipleChoice were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'classifier.weight', 'prefix_encoder.weight', 'embeddings.position_embeddings.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** total param is 175873\n",
      "** train bert? False\n",
      "train batch number: 123 \n",
      "eval batch number: 31\n"
     ]
    }
   ],
   "source": [
    "model = BertPromptForMultipleChoice.from_pretrained(base_model, config)\n",
    "model = model.to('cuda')\n",
    "\n",
    "## weight decay (in optimizer)\n",
    "## -----------------------------\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                        lr=lr, \n",
    "                        weight_decay = wd, # ÊúâÂÖ©Á®Æweight decayÊñπÊ≥ï\n",
    "                        eps = 1e-08) # 1e-08, training args default \n",
    "\n",
    "##  \n",
    "##  scheduler/warmup\n",
    "##  ------------------\n",
    "##  https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L233\n",
    "# num_warmup_steps = warmup_ratio * epochs * len(train_loader)\n",
    "# num_tr_steps = epochs * len(train_loader)\n",
    "# scheduler = get_scheduler(scheduler_type, optimizer, \n",
    "#                           num_warmup_steps = num_warmup_steps, \n",
    "#                           num_training_steps = num_tr_steps)\n",
    "n_train_batch, n_eval_batch = len(train_loader), len(eval_loader)\n",
    "print('train batch number:', n_train_batch, '\\neval batch number:', n_eval_batch)\n",
    "\n",
    "##\n",
    "## loss metric\n",
    "## ----------------\n",
    "train_loss_mean = MeanMetric()\n",
    "eval_loss_mean = MeanMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6770fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ab8628e8b8480f86d0939ef6ad3a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEval Acc: 0.4797\n",
      "* \tSaving model at eval accuracy: 0.4796747967479675.\n",
      "\tEval Acc: 0.5122\n",
      "* \tSaving model at eval accuracy: 0.5121951219512195.\n",
      "\tEval Acc: 0.5041\n",
      "\tEval Acc: 0.4634\n",
      "\tEval Acc: 0.4878\n",
      "\tEval Acc: 0.4878\n",
      "\tEval Acc: 0.5122\n",
      "\tEval Acc: 0.4634\n",
      "\tEval Acc: 0.4715\n",
      "\tEval Acc: 0.4878\n",
      "\tEval Acc: 0.4878\n",
      "\tEval Acc: 0.5203\n",
      "* \tSaving model at eval accuracy: 0.5203252032520326.\n",
      "\tEval Acc: 0.4715\n",
      "\tEval Acc: 0.4715\n",
      "\tEval Acc: 0.4715\n",
      "\tEval Acc: 0.4634\n"
     ]
    }
   ],
   "source": [
    "# *****************\n",
    "#    Train Loop\n",
    "# *****************\n",
    "\n",
    "loss_vec, eloss_vec = [], []\n",
    "best_eval_acc = 0 \n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in range(epochs):\n",
    "    # -----------------\n",
    "    #   Training\n",
    "    # -----------------\n",
    "    # model.train()   \n",
    "    for i, batch in enumerate(train_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: batch[k].to('cuda') for k in batch}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        train_loss_mean.update(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()                \n",
    "        # scheduler.step() ## is the scheduler by-batch step or by-epoch step??\n",
    "    loss_vec.append(train_loss_mean.compute().item())\n",
    "    train_loss_mean.reset()\n",
    "   \n",
    "    \n",
    "    # -------------\n",
    "    #  Evaluating\n",
    "    # -------------    \n",
    "    # model.eval()\n",
    "    all_labels, all_logits = [], []\n",
    "    for i, batch in enumerate(eval_loader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: batch[k].to('cuda') for k in batch}\n",
    "            out = model(**batch)\n",
    "            loss = out.loss            \n",
    "            eval_loss_mean.update(loss.item())\n",
    "            logits = out.logits\n",
    "            labels = batch['labels'].cpu().detach().numpy()\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            all_labels.append(labels)\n",
    "            all_logits.append(logits)\n",
    "    \n",
    "    eloss_vec.append(eval_loss_mean.compute().item())      \n",
    "    eval_loss_mean.reset()\n",
    "\n",
    "            \n",
    "    #\n",
    "    # Computing metrics and logging\n",
    "    # ----------------------------------\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Train/Eval Loss: {loss_vec[-1]:.2f}/{eloss_vec[-1]:.2f}\")\n",
    "    # pbar.set_description(f\"Train/Eval Loss: {loss_vec[-1]:.2f}/----\")\n",
    "    \n",
    "    # concatenating the logits    \n",
    "    all_labels = np.concatenate(all_labels, axis = None)\n",
    "    all_logits = np.concatenate(all_logits, axis = None)\n",
    "    eval_acc, eval_acc_dict = ComputeMetrics()(all_logits, all_labels)\n",
    "    print(f'\\tEval Acc: {eval_acc:.4f}')\n",
    "    if best_eval_acc < eval_acc:\n",
    "        print(f'* \\tSaving model at eval accuracy: {eval_acc}.')\n",
    "        # model.save_pretrained(model_dir)\n",
    "        best_eval_acc = eval_acc\n",
    "#     wandb.log({\n",
    "#             \"train loss\": tr_loss/n_train_batch,\n",
    "#             \"eval loss\": ev_loss/n_eval_batch,\n",
    "#             \"eval acc\": eval_acc,\n",
    "#         })\n",
    "    # for k, v in eval_acc_dict.items():\n",
    "    #     print(f'*\\tacc|numchoice = {k}: {v:.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5bdc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4cb2d0fb50>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_vec)\n",
    "plt.plot(eloss_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62deae6-92ac-4892-9a12-afb514d34a0a",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model.train()Âíåmodel.eval()Á´üÁÑ∂ÂΩ±Èüø„Ñåupdates <-Ê≤íË®≠ÁöÑÊôÇÂÄô‰∏ÄÂàáÊ≠£Â∏∏ÔºåÊúâË®≠Â∞±Ê≤íÊ≥ïbackward()??\n",
    "#                                                 ÊÑüË¶∫‰∏ãÈôçÂπÖÂ∫¶ËÆäÊÖ¢Ôºà15 epochsÂÖßÂè™ËÉΩ0.71 -> 0.33ÔºåÈôç0.2~0.3Ôºâ\n",
    "#                                                 ËÄå‰∏îÂπÖÂ∫¶ÊäñÂãïÊØîËºÉ‰∏çÁ©©ÂÆö\n",
    "# 2. Âä†gradient accumulation stepsÔºàÁîöËá≥Âè™Èñã1‰πüÊúâÂ∑ÆÔºâÔºå‰πüÊòØ 15 epochsÂÖßÂè™ËÉΩ0.71 -> 0.417\n",
    "# 3. labels=torch.tensor(labels, dtype=torch.float32).to(self._device)\n",
    "#    vs. sourceTensor.clone().detach()\n",
    "# 4. Âä†ÂÖ•gradient clipping (Á¨¨‰∫åÂ±§for Ëø¥ÂúàÂÖß)Áõ¥Êé•ÁàÜÊéâÔºåÊï¥ÂÄãlossÂÆåÂÖ®‰∏çÊúÉÂãï\n",
    "# 5. Âä†scheduler‰∏ãÈôçÂπÖÂ∫¶Ë∂ÖÁ¥öÊÖ¢Ôºå15 epochsÂÖßÂè™ËÉΩ0.75 -> 0.67\n",
    "# ÊúÄÂ•ΩÁöÑÁãÄÊÖãÂ§ßÊ¶ÇÊòØeval acc = 0.53Ôºå‰∏çÂä†schedulerÂíågrad-clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9324690",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gradient-clipping'''\n",
    "# if hasattr(optimizer, \"clip_grad_norm\"):\n",
    "#     # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n",
    "#     optimizer.clip_grad_norm(max_grad_norm)\n",
    "# elif hasattr(model, \"clip_grad_norm_\"):\n",
    "#     # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n",
    "#     model.clip_grad_norm_(max_grad_norm)\n",
    "# else: # fall back to normal torch utils\n",
    "#     nn.utils.clip_grad_norm_(\n",
    "#         model.parameters(),\n",
    "#         max_grad_norm,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025291f",
   "metadata": {},
   "source": [
    "## To-add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
