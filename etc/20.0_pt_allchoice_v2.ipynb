{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6989b187",
   "metadata": {},
   "source": [
    "## Loading Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be03755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server Paths \n",
    "# ..data/avo727/PromptTuning/CWNdata/Sean_PT2_encoded_dataset\n",
    "maindir = \"/mnt/md0/data/avo727/PromptTuning\"\n",
    "datadir = f\"{maindir}/CWN_data\"\n",
    "preddir = f\"{maindir}/model_predictions\"\n",
    "datasetdir= f\"{maindir}/CWNdata/PT2_allchoice_encoded_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1fc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset, load_metric\n",
    "import datasets\n",
    "from transformers import AutoModelForMultipleChoice, BertTokenizerFast\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "encoded_dataset = datasets.load_from_disk(datasetdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6317182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset['test']['numchoices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f595b9",
   "metadata": {},
   "source": [
    "## Data Collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e38121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5314ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    allchoice version: ÂÖ®Êï∏Êî§Âπ≥ÈÄÅÂæÄmodelÂÖßÈÉ®ËôïÁêÜÔºà‰∏çÁ∂ìunflattenÔºåÁ¢∫‰øùÂÇ≥Ëº∏numchoicesÔºâ\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizer\n",
    "    padding, trunc = True, True\n",
    "    max_length =  None\n",
    "    pad_to_multiple_of = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        pin_label = True if \"label\" in features[0].keys() else False\n",
    "        accepted_keys = [\"input_ids\", \"attention_mask\", \"label\", \n",
    "                         \"token_type_ids\", 'class_selector', 'numchoices']\n",
    "        batch_size = len(features)\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        numchoices = [feature.pop('numchoices') for feature in features]\n",
    "        seq_classes = [feature.pop('class_selector') for feature in features]\n",
    "        \n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items() if k in accepted_keys} \n",
    "                               for i in range(nc)] for feature, nc in zip(features, numchoices)]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding= \"longest\",\n",
    "            max_length= self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # processing labels: \n",
    "        proclabels = []\n",
    "        for l, nc in zip(labels, numchoices):\n",
    "            label = [0]*nc\n",
    "            label[l] = 1\n",
    "            proclabels.extend(label)\n",
    "        batch = {k: v for k, v in batch.items() if k in accepted_keys}\n",
    "        assert len(proclabels) ==  len(batch['input_ids'])\n",
    "        # print(len(batch['input_ids'])) # print(sum([len(x) for x in seq_classes])) # should match \n",
    "        # flattening list of lists\n",
    "        batch[\"class_selector\"] = torch.tensor(sum(seq_classes, [])) \n",
    "        batch[\"labels\"] = torch.tensor(proclabels, dtype=torch.int64)\n",
    "        batch[\"numchoices\"] = torch.tensor(numchoices, dtype=torch.int64)\n",
    "        # all flattened into instances (instead of problems)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5542549",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b99676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( predictions: typing.Union[numpy.ndarray, typing.Tuple[numpy.ndarray]]\n",
    "# label_ids: typing.Union[numpy.ndarray, typing.Tuple[numpy.ndarray]] )\n",
    "class ComputeMetrics:\n",
    "    from datasets import Dataset, load_metric\n",
    "    datasetdir= f\"{maindir}/CWNdata/PT2_allchoice_encoded_dataset\"\n",
    "    encoded_dataset = datasets.load_from_disk(datasetdir)\n",
    "    numchoices = encoded_dataset['test']['numchoices']\n",
    "    def __call__(self, logits, labels):\n",
    "        # eval_batch_size = len(encoded_dataset['test'])\n",
    "        # È°åËôüÔºålabels(ÈÇÑÂéü)ÔºåÈ†êÊ∏¨label \n",
    "        from collections import defaultdict \n",
    "        mapping, label_mapping, prediction_mapping = defaultdict(list),defaultdict(list), defaultdict(list)\n",
    "        curstart = 0\n",
    "        accu = defaultdict(int)\n",
    "        accdict = {}\n",
    "        for exid, nc in enumerate(self.numchoices):\n",
    "            label_id = labels[exid]\n",
    "            pred_id = np.argmax([logits[i] for i in range(curstart, curstart+nc)])\n",
    "            mapping[nc].append(exid) \n",
    "            label_mapping[nc].append(label_id)\n",
    "            prediction_mapping[nc].append(pred_id)\n",
    "            if label_id == pred_id: \n",
    "                accu[nc] += 1\n",
    "            # update \n",
    "            curstart = curstart+nc\n",
    "        cnt_numchoices = len(mapping) # 2\n",
    "        # print(f'* There are {cnt_numchoices} numchoices in the dataset.')\n",
    "        # return {'accuracy': (preds == label_ids).astype(np.float32).mean().item()}\n",
    "        for nc, nctotal in mapping.items():\n",
    "            accurates = accu[nc]\n",
    "            accdict[nc] = (accurates/len(nctotal))\n",
    "        accurates = sum(accu.values())\n",
    "        return accurates/len(self.numchoices), accdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e05b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "labels = [0,0,1]\n",
    "np.argmax([labels[i] for i in range(0, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db74dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.2000, 0.3000, 0.4000, 0.1000, 0.5000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([0.5,0.2,0.3, 0.4,0.1, 0.5])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a34ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_id = np.argmax([logits[i] for i in range(0, 3)])\n",
    "pred_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4fc52c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {'2':1, '3':3}\n",
    "sum(x.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05603498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.000\n"
     ]
    }
   ],
   "source": [
    "print(f'{5:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542f6b1",
   "metadata": {},
   "source": [
    "## üçÄ Manual Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d68e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SchedulerType(ExplicitEnum):\n",
    "#     LINEAR = \"linear\"\n",
    "#     COSINE = \"cosine\"\n",
    "#     COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "#     POLYNOMIAL = \"polynomial\"\n",
    "#     CONSTANT = \"constant\"\n",
    "#     CONSTANT_WITH_WARMUP = \"constant_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ba895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparams\n",
    "base_model = 'bert-base-chinese'\n",
    "batchsize = 12\n",
    "prompt_len = n_tokens = 12\n",
    "lr = 5e-4 # 5e-4\n",
    "scheduler_type = \"linear\"\n",
    "wd = 0.005 # best 0.005\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 1 # pytorch trainer default \n",
    "myseed = 1126   # best 1126\n",
    "epochs = 20     # best 20\n",
    "# accumulation_steps = 1 # pytorch trainer default \n",
    "###### task-specific stats ##### \n",
    "nclass = 19\n",
    "TESTSIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "490925e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import MultipleChoiceModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f4d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d6282b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertPromptForMultipleChoice: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPromptForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertPromptForMultipleChoice were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['prefix_encoder.weight', 'embeddings.token_type_embeddings.weight', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** total param is 175873\n",
      "** train bert? False\n"
     ]
    }
   ],
   "source": [
    "from PromptTuningBERT_allchoice_v2 import BertPromptForMultipleChoice\n",
    "config = {\n",
    "    'n_tokens': prompt_len ,\n",
    "    'n_class': nclass,\n",
    "    'train_bert': False,\n",
    "    'to_debug': False,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "model = BertPromptForMultipleChoice.from_pretrained(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7444fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''seeding'''\n",
    "import torch.optim as optim\n",
    "from transformers import set_seed, get_scheduler\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "set_seeds(myseed)\n",
    "set_seed(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0e5208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPromptForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (prefix_encoder): Embedding(19, 9216)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1947e222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scheduler with warm up'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''grad accum steps'''\n",
    "# https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3\n",
    "'''trainer'''\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L1453\n",
    "'''scheduler with warm up'''\n",
    "# https://github.com/huggingface/transformers/blob/198c335d219a5eb4d3f124fdd1ce1a9cd9f78a9b/src/transformers/optimization.py#L75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08581bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\tName of the run: 0328-1616_RPBert_allc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnana2929\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/md0/data/avo727/PromptTuning/runs/wandb/run-20220328_161602-1m9c19wq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nana2929/prompt_tuning_rp_v3/runs/1m9c19wq\" target=\"_blank\">0328-1616_RPBert_allc</a></strong> to <a href=\"https://wandb.ai/nana2929/prompt_tuning_rp_v3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''creating model directory'''\n",
    "from datetime import datetime\n",
    "import os\n",
    "import wandb \n",
    "now = datetime.now()\n",
    "timeprefix = now.strftime(\"%m%d-%H%M\")\n",
    "runname = f'{timeprefix}_RPBert_allc'\n",
    "\n",
    "model_dir = f'{maindir}/runs_v2/{runname}'\n",
    "print('*\\tName of the run:', runname)\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "wandb.init(project='prompt_tuning_rp_v3', name = runname, \n",
    "           entity='nana2929', group=\"no trainer\")\n",
    "wandb.config.update({\n",
    "    'learning_rate':lr,\n",
    "    'batch_size':batchsize,\n",
    "    'weight_decay':wd,\n",
    "    'seed':myseed,\n",
    "    'n_tokens':n_tokens, \n",
    "    'epochs':epochs, \n",
    "    'scheduler': scheduler_type,  \n",
    "})\n",
    "# WANDB_NOTEBOOK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a9b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c555199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch number: 41 \n",
      "eval batch number: 11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler)\n",
    "from PromptTuningBERT_allchoice_v2 import BertPromptForMultipleChoice\n",
    "from tqdm import tqdm \n",
    "\n",
    "train_loader = DataLoader(encoded_dataset['train'], \n",
    "                    shuffle=False, \n",
    "                    collate_fn=DataCollatorForMultipleChoice(), \n",
    "                    batch_size=batchsize)\n",
    "num_warmup_steps = warmup_ratio * epochs * len(train_loader)\n",
    "num_tr_steps = epochs * len(train_loader)\n",
    "'''weight decay (in optimizer)'''\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                        lr=lr, \n",
    "                        weight_decay = wd, # ÊúâÂÖ©Á®Æweight decayÊñπÊ≥ï\n",
    "                        eps = 1e-08) # 1e-08, training args default \n",
    "'''scheduler/warmup'''\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L233\n",
    "# scheduler = get_scheduler(scheduler_type, optimizer, \n",
    "#                           num_warmup_steps = num_warmup_steps, \n",
    "#                           num_training_steps = num_tr_steps)\n",
    "eval_loader = DataLoader(encoded_dataset['test'], \n",
    "                    shuffle=False, \n",
    "                    collate_fn=DataCollatorForMultipleChoice(), \n",
    "                    batch_size=batchsize)\n",
    "n_train_batch, n_eval_batch = len(train_loader), len(eval_loader)\n",
    "print('train batch number:', n_train_batch, '\\neval batch number:', n_eval_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6770fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "*\ttrain loss: 0.7000\n",
      "*\teval loss: 0.6898,  \teval acc: 0.4715\n",
      "* \tSaving model at eval accuracy: 0.4715447154471545.\n",
      "*\tacc|numchoice = 2: 0.460\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 2\n",
      "*\ttrain loss: 0.6764\n",
      "*\teval loss: 0.6927,  \teval acc: 0.4878\n",
      "* \tSaving model at eval accuracy: 0.4878048780487805.\n",
      "*\tacc|numchoice = 2: 0.504\n",
      "*\tacc|numchoice = 3: 0.300\n",
      "epoch 3\n",
      "*\ttrain loss: 0.6778\n",
      "*\teval loss: 0.6862,  \teval acc: 0.4878\n",
      "*\tacc|numchoice = 2: 0.478\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 4\n",
      "*\ttrain loss: 0.6701\n",
      "*\teval loss: 0.6865,  \teval acc: 0.5366\n",
      "* \tSaving model at eval accuracy: 0.5365853658536586.\n",
      "*\tacc|numchoice = 2: 0.513\n",
      "*\tacc|numchoice = 3: 0.800\n",
      "epoch 5\n",
      "*\ttrain loss: 0.6664\n",
      "*\teval loss: 0.6922,  \teval acc: 0.5447\n",
      "* \tSaving model at eval accuracy: 0.5447154471544715.\n",
      "*\tacc|numchoice = 2: 0.522\n",
      "*\tacc|numchoice = 3: 0.800\n",
      "epoch 6\n",
      "*\ttrain loss: 0.6464\n",
      "*\teval loss: 0.6960,  \teval acc: 0.5285\n",
      "*\tacc|numchoice = 2: 0.513\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 7\n",
      "*\ttrain loss: 0.6317\n",
      "*\teval loss: 0.7059,  \teval acc: 0.4959\n",
      "*\tacc|numchoice = 2: 0.469\n",
      "*\tacc|numchoice = 3: 0.800\n",
      "epoch 8\n",
      "*\ttrain loss: 0.6166\n",
      "*\teval loss: 0.7021,  \teval acc: 0.5203\n",
      "*\tacc|numchoice = 2: 0.496\n",
      "*\tacc|numchoice = 3: 0.800\n",
      "epoch 9\n",
      "*\ttrain loss: 0.6008\n",
      "*\teval loss: 0.7144,  \teval acc: 0.5203\n",
      "*\tacc|numchoice = 2: 0.513\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 10\n",
      "*\ttrain loss: 0.5715\n",
      "*\teval loss: 0.7672,  \teval acc: 0.5285\n",
      "*\tacc|numchoice = 2: 0.513\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 11\n",
      "*\ttrain loss: 0.5704\n",
      "*\teval loss: 0.7316,  \teval acc: 0.5285\n",
      "*\tacc|numchoice = 2: 0.513\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 12\n",
      "*\ttrain loss: 0.5493\n",
      "*\teval loss: 0.7553,  \teval acc: 0.5122\n",
      "*\tacc|numchoice = 2: 0.496\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 13\n",
      "*\ttrain loss: 0.5252\n",
      "*\teval loss: 0.7392,  \teval acc: 0.4715\n",
      "*\tacc|numchoice = 2: 0.460\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 14\n",
      "*\ttrain loss: 0.5027\n",
      "*\teval loss: 0.7864,  \teval acc: 0.5203\n",
      "*\tacc|numchoice = 2: 0.504\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 15\n",
      "*\ttrain loss: 0.4692\n",
      "*\teval loss: 0.7926,  \teval acc: 0.5041\n",
      "*\tacc|numchoice = 2: 0.487\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 16\n",
      "*\ttrain loss: 0.4422\n",
      "*\teval loss: 0.8542,  \teval acc: 0.5203\n",
      "*\tacc|numchoice = 2: 0.504\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 17\n",
      "*\ttrain loss: 0.4567\n",
      "*\teval loss: 0.8586,  \teval acc: 0.5122\n",
      "*\tacc|numchoice = 2: 0.504\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 18\n",
      "*\ttrain loss: 0.3910\n",
      "*\teval loss: 0.9145,  \teval acc: 0.5203\n",
      "*\tacc|numchoice = 2: 0.504\n",
      "*\tacc|numchoice = 3: 0.700\n",
      "epoch 19\n",
      "*\ttrain loss: 0.3889\n",
      "*\teval loss: 0.9852,  \teval acc: 0.4797\n",
      "*\tacc|numchoice = 2: 0.469\n",
      "*\tacc|numchoice = 3: 0.600\n",
      "epoch 20\n",
      "*\ttrain loss: 0.3997\n",
      "*\teval loss: 0.8718,  \teval acc: 0.4797\n",
      "*\tacc|numchoice = 2: 0.469\n",
      "*\tacc|numchoice = 3: 0.600\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler)\n",
    "\n",
    "loss_vec, eloss_vec = [], []\n",
    "best_eval_acc = 0 \n",
    "for epoch in range(epochs):\n",
    "    '''training'''\n",
    "    model.train()\n",
    "    tr_loss, ev_loss = 0,0 \n",
    "    for i, batch in enumerate(train_loader):  \n",
    "        batch = {k: batch[k].to('cuda') for k in batch}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        tr_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # scheduler.step()\n",
    "    loss_vec.append((tr_loss/n_train_batch))\n",
    "    '''evaluating'''\n",
    "    print(f'epoch {epoch+1}')\n",
    "    model.eval()\n",
    "    all_labels, all_logits = [], []\n",
    "    for i, batch in enumerate(eval_loader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: batch[k].to('cuda') for k in batch}\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            ev_loss += loss.item()\n",
    "            logits = out.logits\n",
    "            labels = batch['labels'].cpu().detach().numpy()\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            all_labels.append(labels)\n",
    "            all_logits.append(logits)\n",
    "    eloss_vec.append(ev_loss/n_eval_batch)      \n",
    "    # concatenating the logits\n",
    "    '''Computing metrics and logging'''\n",
    "    all_labels = np.concatenate(all_labels, axis = None)\n",
    "    all_logits = np.concatenate(all_logits, axis = None)\n",
    "    eval_acc, eval_acc_dict = ComputeMetrics()(all_logits, all_labels)\n",
    "    print(f'*\\ttrain loss: {(tr_loss/n_train_batch):.4f}' )\n",
    "    print(f'*\\teval loss: {(ev_loss/n_eval_batch):.4f},  \\teval acc: {eval_acc:.4f}')\n",
    "    if best_eval_acc < eval_acc:\n",
    "        print(f'* \\tSaving model at eval accuracy: {eval_acc}.')\n",
    "        # model.save_pretrained(model_dir)\n",
    "        best_eval_acc = eval_acc\n",
    "#     wandb.log({\n",
    "#             \"train loss\": tr_loss/n_train_batch,\n",
    "#             \"eval loss\": ev_loss/n_eval_batch,\n",
    "#             \"eval acc\": eval_acc,\n",
    "#         })\n",
    "    for k, v in eval_acc_dict.items():\n",
    "        print(f'*\\tacc|numchoice = {k}: {v:.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model.train()Âíåmodel.eval()Á´üÁÑ∂ÂΩ±Èüø„Ñåupdates <-Ê≤íË®≠ÁöÑÊôÇÂÄô‰∏ÄÂàáÊ≠£Â∏∏ÔºåÊúâË®≠Â∞±Ê≤íÊ≥ïbackward()??\n",
    "#                                                 ÊÑüË¶∫‰∏ãÈôçÂπÖÂ∫¶ËÆäÊÖ¢Ôºà15 epochsÂÖßÂè™ËÉΩ0.71 -> 0.33ÔºåÈôç0.2~0.3Ôºâ\n",
    "#                                                 ËÄå‰∏îÂπÖÂ∫¶ÊäñÂãïÊØîËºÉ‰∏çÁ©©ÂÆö\n",
    "# 2. Âä†gradient accumulation stepsÔºàÁîöËá≥Âè™Èñã1‰πüÊúâÂ∑ÆÔºâÔºå‰πüÊòØ 15 epochsÂÖßÂè™ËÉΩ0.71 -> 0.417\n",
    "# 3. labels=torch.tensor(labels, dtype=torch.float32).to(self._device)\n",
    "#    vs. sourceTensor.clone().detach()\n",
    "# 4. Âä†ÂÖ•gradient clipping (Á¨¨‰∫åÂ±§for Ëø¥ÂúàÂÖß)Áõ¥Êé•ÁàÜÊéâÔºåÊï¥ÂÄãlossÂÆåÂÖ®‰∏çÊúÉÂãï\n",
    "# 5. Âä†scheduler‰∏ãÈôçÂπÖÂ∫¶Ë∂ÖÁ¥öÊÖ¢Ôºå15 epochsÂÖßÂè™ËÉΩ0.75 -> 0.67\n",
    "# ÊúÄÂ•ΩÁöÑÁãÄÊÖãÂ§ßÊ¶ÇÊòØeval acc = 0.53Ôºå‰∏çÂä†schedulerÂíågrad-clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e5bdc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff40bb49340>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArOklEQVR4nO3deXxU5dn/8c81kz0kkJCEQEISdgiCggFEEHABQS2IWgW1uONSbetjFx99nj6tbX/d7KKtVdG6r7hjRQHRAlJAAohIEAhhSwhZSEggezL3748zkTELGcjMnCzX+/Wa1yznnnMuhsk3J/c5577FGINSSqnOz2F3AUoppXxDA10ppboIDXSllOoiNNCVUqqL0EBXSqkuIsiuDcfFxZm0tDS7Nq+UUp3Spk2bio0x8S0tsy3Q09LSyMzMtGvzSinVKYnI/taWtdnlIiLPiEihiHzVynIRkUdFJFtEvhSRse0pViml1Onxpg/9OWDmSZbPAoa4bwuBx9tfllJKqVPVZqAbY1YDJSdpMgd4wVjWA71EpK+vClRKKeUdX5zlkgQc9Hie636tGRFZKCKZIpJZVFTkg00rpZRqFNDTFo0xi4wxGcaYjPj4Fg/SKqWUOk2+CPQ8oL/H82T3a0oppQLIF4G+BFjgPtvlHKDMGJPvg/UqpZQ6BW2ehy4irwLTgDgRyQX+DwgGMMY8ASwFLgGygUrgJn8Vq5RStnI1wFdvwdCLIayn3dU002agG2Pmt7HcAN/3WUVKKdVRrf8HLP8fmHAnzPqd3dU0o2O5KKWUNwp3wMpfgSMIvngZao7bXVEzGuhKKdWWhjp453YI7QFXvwg15bD1VburakYDXSml2rL6YcjfCpf9FYbNgn5j4PNF0MGm8NRAV0qpk8nbDKv/CKOvgfTZIALjb4fiXZDzqd3VfYsGulJKtaauCt65A3r0gVl/OPH6GVdARBxsWGRfbS3QQFdKqdas/BUU74TLH4PwXideDwqFs2+EXR9ByV67qmtGA10ppVqy7zPrNMVxt8KgC5ovH3cLiAM2Ph342lqhga6UUk3VHIN374TYATD9oZbbRPez+tS3vAi1FYGtrxUa6Eop1dSyB6AsFy5/AkIiW283/naoLoMvFweutpPQQFdKKU+7lsHmF+DcH0DKhJO3TTkHEkd1mFMYNdCVUqpRZQksuQcSRsL5D7TdvvEUxsIsq8/dZhroSinV6IP7rFCf+4R1Jos3Rl0F4bGw4Qn/1uYFDXSllAJrFMXtb8O0n0Hf0d6/Lzgcxi6AnUvh6AH/1ecFDXSllDp22No7T8qASfee+vvH3Wrdb/ynb+s6RRroSqnuzRir37yu2upqcbY5qnhzvfrD8Eth8/PW1aU20UBXSnVvm5+H3cth+i8hbsjpr2f87VBVCtve9F1tp0gDXSnVfZXug2UPwoApMO629q0rbTIkpMPnT9p2CqMGulKqe3K54N27rMv35/wDHO2MQxEYvxAOb4MD631T4ynSQFdKdU/r/wH718LM31l94L4w+mprrlGbTmHUQFdKdT+FX8PKh2DYJXDWtb5bb0ikdQrjjvehLM936/WSBrpSqnvxnE7uO49YXSW+NO5WMC7IfMa36/WCBrpSqntZ8yfI/wIu+wv0SPD9+mPSrGnqNj1nnQoZQBroSqnuI2eVNZ3cqKshfY7/tjN+IVQWw/Z3/LeNFmigK6W6h8Kv4fXvQe8hcOnD/t3WwGkQNyzgpzB6FegiMlNEdopItojc38LyVBFZKSJfisi/RSTZ96UqpdRpOl4Ir3wXgsPgusXWmSj+JALjb4NDWyA307/b8tBmoIuIE3gMmAWkA/NFJL1Js4eBF4wxo4GHgN/6ulCllDottZXwyjVQUQzzX4NeKYHZ7pnzITTa2ksPEG/20McD2caYHGNMLfAa0LTzKR34xP340xaWK6VU4Lka4G33nvKVT0PS2MBtO7QHjLne6kc/djggm/Qm0JOAgx7Pc92vedoKXOF+PBeIEpHeTVckIgtFJFNEMouKik6nXqWU8t6Kn8PX/4KZv7UGzwq0cbdav1Qynw3I5nx1UPTHwFQR2QJMBfKAhqaNjDGLjDEZxpiM+Ph4H21aKaVa8PlTsO7v1qBZ59xpTw29B8GQ6bDpWaiv9fvmvAn0PMDzuthk92vfMMYcMsZcYYwZAzzofu2or4pUSqlTsmsZfPhTGDrL2ju30/jb4XgBZL3n9015E+gbgSEiMkBEQoB5wBLPBiISJyKN6/pvIPCXSCmlFMChL+CNm6zJm698GhxOe+sZdAHEDgrIwdE2A90YUw/cDSwDdgCLjTHbReQhEZntbjYN2Ckiu4A+wG/8VK9SSrWuLNc6oyU8Bq5dbB2YtJvDYV1olLsR8jb7dVNibBq3NyMjw2RmBu78TKVUF1ddDs/MhLKDcPNH0Gek3RWdUF0Ofx4BI75jzYrUDiKyyRiT0dIyvVJUKdX5NdTBGzdC8U64+vmOFeYAYdHWqI5fvQXH/XeGnwa6UqpzMwaW/hj2rLQG3Bp0gd0VtWz8QmiotQbt8hMNdKVU57b2ESskJ/+XNRZ5RxU3xPplk/lP6y8KP9BAV0p1XtvfgY//D0ZeARf8r93VtG387XAs35oAww800JVSndPBz+Ht26H/OXD54+2fEzQQhsyw/orw03gyQX5Zq1JK+VNJDrw6D3omwbxXrFEUOwOHA2b/zX+r99ualVLKHypL4OXvWtO8XfcmRDYbNqrb0j10pVTH5Gqwwrui0BrPvKLIut/+Dhw9AAuWWGOlqG9ooCulAquyxLr453hR87CuKHS/XmRN4WZczd8fHGldnJM6MfC1d3Aa6EqpwMleafV9NzQZeTAoHHrEQ2QCxKRCcoY1gXNkAkTGnXjcIx7CelkzAqlmNNCVUoFRuh/eugV6D4Zp/+0O6XjrPqSHhrQPaKArpfyvrhoWfw9cLrjmJe379hMNdKWUfxkDS++D/K3WnJ4a5n6jpy0qpfxr03Ow5SWY8hMYNsvuaro0DXSllP/kbrJmDhp0odVvrvxKA10p5R/Hi6x+86jEjjFzUDegfehKKd9rqIc3b4LKI3DLcoiItbuibkEDXSnle588BPvWwJx/QN8z7a6m29AuF6WUb2W9Z41RnnEzjLnO7mq6FQ10pZTvFO2Ed++CpAyY+Tu7q+l2NNCVUr5Rcwxevx6Cw+HqFyAo1O6Kuh3tQ1dKtZ8x1p75kT2w4D1rnHIVcBroSqn2W/sI7FgC038FA86zu5puS7tclFLtk7MKVv4S0i+Hc++xu5puzatAF5GZIrJTRLJF5P4WlqeIyKciskVEvhSRS3xfqlKqwynLtc437z0E5vxdR0y0WZuBLiJO4DFgFpAOzBeR9CbN/gdYbIwZA8wD/uHrQpVSHUx9DSxeAPW1MO9lCI2yu6Juz5s99PFAtjEmxxhTC7wGzGnSxgDR7sc9gUO+K1Ep1SF9+FPI2wRzH4e4IXZXo/DuoGgScNDjeS4woUmbXwDLReQeIBK4yCfVKaU6ps0vWqMoTvoRjPiO3dUoN18dFJ0PPGeMSQYuAV4UkWbrFpGFIpIpIplFRUU+2rRSKqDyNsMH98GAqXDB/9pdjfLgzR56HtDf43my+zVPtwAzAYwx60QkDIgDCj0bGWMWAYsAMjIyzGnWrJQKlNpKOHoASvfB0f3WNHLb37GmjrvqGXDqmc8diTf/GxuBISIyACvI5wHXNmlzALgQeE5ERgBhgO6CK9XRNdRDea4V1I2B7RneFYXfbh8UDnGDYfbfrMmbVYfSZqAbY+pF5G5gGeAEnjHGbBeRh4BMY8wS4D7gKRG5F+sA6Y3GGN0DV6qjqSyBf/8OinZYgV2WC6bhxHJxWld59kqFoRdDTCr0SnPfp1oTOuupiR2W2JW7GRkZJjMz05ZtK9UtVRTDC3OsAbT6nWUFdGNQx6RCTBpEJ4Ez2O5K1UmIyCZjTEZLy7QDTKnu4FgBvDDb6k65bjEMusDuipQfaKAr1dWV58Pz34HyPLjuDRgwxe6KlJ9ooCvVlZXlWmF+vBCufwtSz7W7IuVHGuhKdVWl+60wryqF770D/cfbXZHyMx1tUXUvLhcUZFnjd3dlJTnw3KVQfRQWvKth3k1ooKvuwxj46H54fKI1s05lid0V+UdxNjx7KdQehxveh6Sz7a5IBYgGuuo+1jwMnz8JA6fBrmXw5BQ4sMHuqnyraCc8dwk01MIN/4K+Z9pdkQogDXTVPWQ+C5/8GkZfA9e/A7csA3HAs7NgzZ+srpjOrmA7PHuJ9ZfIjR9A4hl2V6QCTANddX1Z78EH/wVDZsCcx8DhsLoh7lgD6bNh5UPw0lzrXG1/a6iDgxuhtsK3683fCs9dZl0UdNNSSBju2/WrTkHPclFdW84qeOtWSB4H333+21dBhvWEq561umA+/Bk8MQmuWOSfi27qa+CLl+Gzv1iDXQWFw9AZ1rRtQy+GkMjTX3feJnhxLoREwQ1LoPcgn5WtOhcNdNV1HdoCr10LsYNg/msQEtG8jQicfSMkj7emUnvxCph8L5z/gG8uga+rhs0vwNq/Whf2JGXA1Put2rLes27tCfeDn8NLV0J4L+sAaExa+2tWnZaO5aK6piN74J8zIDgcblkO0f3afk9tJXz0MyuA+0+AK5+GXimnt/3aCqvf/j+PwvECSDkXpv4EBp5/YnArVwMcWGcNR5u1xBrZ8FTCff86ePkqayjbG96HXv1bb6u6jJON5aKBrrqe8nx4ZoYVqjcvO/Xp0ba9Ce//yOprn/PYqc3IU3MMPn8K1j0GlcXWZfZTfwZpk0/+vlMN971r4JWrrV9UN7zv3S8s1SVooKvuo6rUOtPj6AH3OdhjT289JTnw5s1W18i422DGryE47CTbPQqfL4L1/7BqGHwRTPkppDSdrdELrgbY/x/IerflcA8OhzduskZIXLAEovqc3r9RdUoa6Kp7qK20Dg7mbbIGoRp0fvvWV18LK38J6/4OfUbBd59tvrdfWWKF+IYnoaYchs6yulZ8dTFPS+EOkDASFrwHPeJ9sx3VaWigq66voc66+nPXMit4R8713bp3LYN37rDOVLn0T3DWfDheBOv+Bhv/aV2ROWI2TPkJ9B3tu+021RjuB9bDuFsgItZ/21Idlga66tqMgXfvgq2vWIE77lbfb6P8ELx1G+z/DNLOg9xMqK+GM66EKT+GhBG+36ZSLdAJLlTXtuLnVphPe8A/YQ7ug49LYPUfrXPJR86F8+479QOuSvmRBrrq3NY+Yp0aOO42mPpT/27L4YRp91sHOx16kbXqePRbqTqvLS9be+cjr4BZfwjc5MUa5qqD0m+m6py+XgpL7rEu1Jn7pIasUmigq85o/3+sy/T7ngnXvAhBIXZXpFSHoH3oqnOorbCupNy7xrqkvmd/uO5NCI2yuzKlOgwNdNUx1VVZA0/tW2OFeF4muOrBEQQpE+HyxyGyt91VKtWhaKCrjqG+1rrCc98a2LvaCvOGGmsSin5j4Nx7rPO/U85p31CzSnVhXgW6iMwEHgGcwNPGmN81Wf4XoPE66wggwRjTy4d1Kru4XFC6F/I2W+OaHNoCJXsgNBoi4yCit3XFYkRviGh87r5Fuu9DejQ/A6WhHvK/sMJ73xrr6se6SkAgcRSMv80K8NSJ1rjlSqk2tRnoIuIEHgOmA7nARhFZYozJamxjjLnXo/09wBg/1ApATX0D9Q2GyFD948LnjLEGtTrkEd6HtkJNmbU8KNy6tH3IdKtPu/IIlOy1rpqsPAKuupbX6wzxCPxY63zugxuh9pi1PH4EjPkeDDgPUifpJe1KnSZvUnE8kG2MyQEQkdeAOUBWK+3nA//nm/KaW7zxIH9asYtbJw9gwblpRIf5YBKC7sgYOJb/7T3vQ1ugqsRa7gi25qQcdZXV5dFvDMQPB2crXxljrKFjK4utAasqj0BFsXXf9FZXZa13wBRrL1wHmFLKJ7wJ9CTgoMfzXKDFMUFFJBUYAHzSyvKFwEKAlJTTmzjgrP4xjE2J4eHlu1i0OocbJw3g5klp9IrQU9e+UVftDtbGUC359vPyQ1Z3x3H3HJrihIR0GH6pFdxJY63nQaHeb1MEwqKtW+xAv/yzlFIn5+t+i3nAm8aYhpYWGmMWAYvAGpzrdDYwKrknz9w4jm25Zfztk908unI3z3y2lwUTU7l18gBiQ11QXQ7VZdZwpp731eXWY1e9ddpbTJp165Xim+nG/KnmuDWF2dGD1r3nHm9FsUdgH4G6ViYgFgeEx0KPPta8mY173n3OaHl6NqVUp+JNoOcBnnNbJbtfa8k84PvtLeqksj+Gr95hVE0Zi0wZVcmlHCsrwbHuGFHrKkBa/F3iQaw+XFe9x0sO6Jl8IuBjBng8TvN/n67LZe0tl+VC2UH3feNj9/Oq0ubvCwr3ODDZG+KGNjkoGXfiQGVknHVw0eH0779FKWUbbwJ9IzBERAZgBfk84NqmjURkOBADrPNphU2V7oOcT62zLMKiCY/pS3jiMMpMOKvy69lS6KJCIhk1OIULRg8mJtYdZGHR1ntCeljrOZZvrat0r/vefft6qbW36yms57cDPjoJEMBYfcfGdeJx0/tmy7CGXS3P8wjtvOYHFEOirDkieyZbM9b37O++JVsj/0XG6161UupbvBoPXUQuAf6KddriM8aY34jIQ0CmMWaJu80vgDBjzP3ebNhf46HvLa7gsU+zeWdLHk4Rrh6XzJ3TBpPUK9z7ldQcg9L9LQd+6f7Wz+bwljggqp8Vzt+69T8R4nqqnlKqBd1ygosDRyp5fFU2b27KBeDKscncNW0wKb3buVfravDo/hDrYKDIicdN78XRwmtOHUxKKXVaumWgN8o7WsWTq/bw2saDNLgMl5+VxIKJqYQEOaitd1FT76KmvoGaOhe1DSce19S73Msb3G1c1NQ1UNvgAoQBcREMTujB4PgokmPCcTgCNHSrUqpb69aB3qigvJonV+Xwyuf7qa5zndJ7RSAsyElosIMQp4N6l6Gkovab5WHBDgbG9bAC3uOW1juSkCDdE1dK+Y4GuoeiYzWszzlCkEMIDXYQGuQkNMi6DwlyWI+Dv/08yCFIk0vXj1bWkl14/MStyLrPLa36po3TIaTGRjCoMeTjezCkTw8GxEXSIzSo2TqVUqotGugBVFXbwJ6i4+xxB/zuAivs9xVXUO868VmHOB3ERAYTExFCbGQIMREhxEQGExsRQkyk9VqviBD382BiI0MID3bqLwGlujmdJDqAwkOcnJHUkzOSvn2WSl2Di/1HKskuPM6BkgpKKuoorailpLKW0opadhwu52hlHaWVtbT2OzYkyEFsRAh9e4Uxd0wSc8ckEaVDHyil3HQPvYNpcBnKq+q+CfqSilqOVn77eVZ+OdsPlRMR4uTyMUlcPyGV9H7RdpeulAoA3UPvRJwOISbS6nahlTGrjDFszS3jpfX7eWtTLq9sOMDZqTF875xUZo1KJDRIrwZVqjvSPfRO7mhlLW9uyuWl9fvZd6SS2MgQrs7oz3UTUugfq1eSKtXV6EHRbsDlMqzdU8yL6/bz8Y4CDDBtaDzXn5PKtGEJOPU8eaW6BA30bubQ0Spe+/wAr248SNGxGpJ6hXPthBSuGdefuB6nMCSuUqrD0UDvpuoaXCzfXsBL6/ezLucIwU5h1hl9uW5CCuPSYvXqVqU6IQ10RXbhMV5af4C3NudyrLqeuB6hXDg8genpfZg0OI7wED2QqlRnoIGuvlFZW8+KrAJWZBWwamcRx2rqCQt2MHlwPDPS+3D+8ATio7RbRqmOSk9bVN+ICAlizllJzDkridp6Fxv2HuFjd8B/vKMAERjTvxfT0xOZnp7AoPgeenWqUp2E7qErwDq3PSu/nI+zClmx4zBf5ZUDMCAukotGJHDRiD6cnRpDkFMHG1PKTtrlok5ZflkVH+8o5OOsAtbtOUJtg4uYiGDOH57A9BF9mDI0nshQ/QNPqUDTQFftcqy6jjW7i1mRVcAnXxdSVlVHSJCD8wbHMWNkHy4c0UdPh1QqQDTQlc/UN7jYuK+U5VmHWb69gLyjVYhARmoMM9ITmZ7eh7S4SLvLVKrL0kBXftHY7758u3VQNSvf6ncf1ieKGSP7MCM9kTOSovWgqlI+pIGuAuJgSSUrsgpYnnWYz/eW4DLQt2cY09OtcJ8wMJZgPaiqVLtooKuAK6mo5ZOvC1m+/TCrdxdRXeciOiyIC4YnMGNkItOGxRMRogdVlTpVGujKVlW1DazZXcTyrAJW7iigtLKOsGAH04YmMGtUIhcMT9CJOpTykl5YpGwVHuJkxshEZoxMpL7Bxef7Svjoq8PWbfthQpwOpgyNY9YZfbloRB96Rmi4K3U6dA9d2cblMmw+UMrSbYf56Kt8DpVVE+wUzh0UxyWjEpmenkhsZIjdZSrVobS7y0VEZgKPAE7gaWPM71poczXwC8AAW40x155snRroylPjLEwfbstn6Vf5HCypwukQJg7szcwzErl4ZKKOMaMU7Qx0EXECu4DpQC6wEZhvjMnyaDMEWAxcYIwpFZEEY0zhydarga5aY4xh+6FyPvwqn6XbDrO3uAIRGJ8WyyWj+jLzjET6RIfZXaZStmhvoE8EfmGMudj9/L8BjDG/9WjzB2CXMeZpb4vSQFfeMMaws+AYH247zIdf5bOr4DgOgQUT0/ivGUOJ1oOpqptp70HRJOCgx/NcYEKTNkPdG1qL1S3zC2PMRy0UshBYCJCSkuLFplV3JyIMT4xmeGI0904fSnbhcZ5du5fn1+3jg235PHjJCOac1U8vXlIK8NVVHkHAEGAaMB94SkR6NW1kjFlkjMkwxmTEx7cypb1SJzE4oQe/mTuK974/iX49w/jR618w/6n17C44ZndpStnOm0DPA/p7PE92v+YpF1hijKkzxuzF6nMf4psSlWpudHIv3r5rEr+ZewY78o8x65E1/PbDHVTU1NtdmlK28SbQNwJDRGSAiIQA84AlTdq8i7V3jojEYXXB5PiuTKWaczqE6yak8sl9U5k7JoknV+Vw0Z9X8eG2fOw6HVcpO7UZ6MaYeuBuYBmwA1hsjNkuIg+JyGx3s2XAERHJAj4FfmKMOeKvopXy1LtHKH/87pm8ecdEeoYHc+fLm7nh2Y3sLa6wuzSlAkovLFJdSn2DixfW7efPK3ZRW+/ijmmDuGvaIMKCdRJs1TWc7CwXHfpOdSlBTgc3Tx7AJ/dNZdaoRB5duZvpf1nFJ18X2F2aUn6nga66pIToMB6ZN4ZXbptAaJCTm5/L5LYXMsktrbS7NKX8RrtcVJdXW+/in5/t5dGVuzEYFp43kEEJPQhxOgh2OggOchDsEOve6SDYKd8sC/J4bC0Xgh0OHA49713ZQ0dbVN1aSJCDO6cNYvZZ/fj1v7J49JPs9q3P6eCy0X25adIARiX39FGVSrWf7qGrbie/rIqKmgbqXS7q6g21DS7q3Lf6hm8/r6s31Llc1NW7qHMvyztaxXtb8qiobWBcWgw3TxrA9PQ+BOlsTCoAdIILpXysvLqOxRsP8vy6fRwsqSKpVzgLJqYyb1yKjueu/EoDXSk/aXAZPt5RwLNr97I+p4TwYCdXnp3EjecOYHBCD7vLU12QBrpSAZB1qJxn1+7lva2HqK13MXVoPDdPHsCUIXE6eJjyGQ10pQKo+HgNr2w4wIvr91N0rIZB8ZHcNGkAV4xN0omxVbtpoCtlg9p6Fx9sO8Sza/fxZW4Z0WFBzB+fwoJz00jqFW53eaqT0kBXykbGGDbtL+XZtfv4aPthAOaOSeLu8weTFhdpc3Wqs9Hz0JWykYiQkRZLRloseUer+Oeavby8YT/vbMnTYFc+pXvoStmg8Fg1T67K4aX1+6l3GQ125TXtclGqg9JgV6dKA12pDk6DXXlLA12pTkKDXbVFA12pTqawvJonVuXw8gYNdvVtGuhKdVIa7KopDXSlOrmmwX7Z6L7cPmUQ6f2i7S5NBZgGulJdRGF5NU+tyeGVDQeoqG1g6tB47pg6iHMGxup4Md2EBrpSXUxZZR0vbdjPs2v3Uny8ljOTe3LH1EHMGJmIU2dT6tI00JXqoqrrGnhzUy6LVudwoKSSgXGR3DZlIFeMTSI0yGl3ecoPNNCV6uIaXIYPv8rniVV7+CqvnPioUG6eNIDrzkkhOkwn3OhKNNCV6iaMMazNPsITq/bwWXYxUaFBXHtOCrdMGkBCdJjd5SkfOFmgezUJoojMFJGdIpItIve3sPxGESkSkS/ct1vbW7RS6tSJCJOHxPHSrRN4/+7JTBkWz1Orc5j8+0+5/60vySk6bneJyo/a3EMXESewC5gO5AIbgfnGmCyPNjcCGcaYu73dsO6hKxUY+4oreGpNDm9syqWuwcXF6Yk8eOkI+sdG2F2aOg3t3UMfD2QbY3KMMbXAa8AcXxaolPKftLhIfjN3FGt/dgF3TRvEZ9nFzPzrahZnHsSuLlflH94EehJw0ON5rvu1pq4UkS9F5E0R6d/SikRkoYhkikhmUVHRaZSrlDpd8VGh/OTi4Xz0o/MYldyTn775JQtf3ETx8Rq7S1M+4lUfuhfeB9KMMaOBFcDzLTUyxiwyxmQYYzLi4+N9tGml1KlIjonglVvP4X8uHcGqnUXM/OtqVmQV2F2W8gFvAj0P8NzjTna/9g1jzBFjTOOv+aeBs31TnlLKHxwO4dbzBvL+PZOJjwrjthcyuf+tLzleU293aaodvAn0jcAQERkgIiHAPGCJZwMR6evxdDaww3clKqX8ZVhiFO9+/1zunDaI1zMPMuuR1WTuK7G7LHWa2gx0Y0w9cDewDCuoFxtjtovIQyIy293sByKyXUS2Aj8AbvRXwUop3woNcvKzmcNZfPtEAK5+ch2//+hrautdNlemTpVeWKSU+sbxmnp+9X4Wr2ceJL1vNH+55iyGJUbZXZby0O4Li5RS3UOP0CB+f9VonlqQQUF5Nd/5+2c8vSYHl0tPb+wMNNCVUs1MT+/DsnunMGVIPL/+YAfXPb2BvKNVdpel2qCBrpRqUVyPUJ5acDZ/uHI0X+YeZeZfVvP25ly9GKkD00BXSrVKRLh6XH8+/OEUhveN4r8Wb+X7r2ymsLza7tJUCzTQlVJtSukdwWsLJ/KzmcNZkVXAtIf/zSMf76ayVs9b70g00JVSXnE6hDunDWLFvVOZOjSev3y8i/Mf/jdvbsrVg6YdhAa6UuqUpMVF8vj1Z/PGHRNJ7BnOj9/Yynf+/hn/2VNsd2ndnga6Uuq0jEuL5Z07z+WReWdxtLKOa5/awK3Pb2SPH8dcr6ipZ92eI9Q16EVPLdELi5RS7VZd18Aza/fyj0/3UF3XwHUTUvjhRUOJjQxp97pzSyv55OtCPt5RyPo9R6htcHHDxFR+OecMH1Te+egUdEqpgCg+XsNfP97Fq58fJCLEyd3nD+bGSWmnNGG1y2X4IvcoK3cUsHJHIV8fPgbAwLhILhyRQElFHW9tzuXJ753NxSMT/fVP6bA00JVSAbW74Bj/b+kOPt1ZRHJMOPfPGs6lo/oiIi22r6ipZ83uYlbuKODTnYUUH6/F6RAyUmO4aEQfLhyRwMD4HgDU1ru46on/sP9IJUt/eB5JvcID+U+znQa6UsoWn+0u5tcfZPH14WOMTenFg5emc3ZqDAB5R6tYuaPgW10p0WFBTBuWwIUjEpg2NIGeEcEtrnf/kQouffQzhiVG8frCcwhydp/DgRroSinbNLgMb23K5eHlOyk8VsP5w+LJL6v+pitlQFwkFw5P4MIRfchIiyHYy3BesvUQP3h1C3efP5gfXzzMn/+EDuVkgR4U6GKUUt2L02FdbXrp6L48uTqHl9fvZ1BCDx64ZDgXjujDIHdXyqmafWY/1u4u5rF/ZzNxUG8mDY7zceWdj+6hK6U6rcraemb/fS1lVXUs/cF5xEeF2l2S3+nwuUqpLikiJIi/XzuGsqo67ntja7e/YlUDXSnVqQ1PjObnl6WzelcRT63JsbscW2mgK6U6vesmpHDJqET+uGwnWw6U2l2ObTTQlVKdnojw2ytG0yc6jHte3UJZVZ3dJdlCA10p1SX0DA/mb9eOIb+smgfe3tYtJ+LQQFdKdRljU2L48YxhfLAtn1c/P2h3OQGnga6U6lJunzKQ84bE8cv3t7PTffFSd6GBrpTqUhwO4c9Xn0VUWDB3v7KZqtoGu0sKGA10pVSXEx8Vyl+vOYvsouP88v3tdpcTMF4FuojMFJGdIpItIvefpN2VImJEpMWrmJRSKlAmD4njzqmDeG3jQZZsPWR3OQHRZqCLiBN4DJgFpAPzRSS9hXZRwA+BDb4uUimlTse904cyNqUXD7y9jQNHKu0ux++82UMfD2QbY3KMMbXAa8CcFtr9Cvg9UO3D+pRS6rQFOx08On8MDoF7Xt1Mbb19U9fVNbjYllvG8//Zx+4C/xys9Wa0xSTA8/yfXGCCZwMRGQv0N8Z8ICI/aW1FIrIQWAiQkpJy6tUqpdQpSo6J4A9XjeaOlzbzx2Vf8+ClzToY/KKkopYtB0rZtL+UzQdK2XqwjKo66wDt/16WzpA+UT7fZruHzxURB/Bn4Ma22hpjFgGLwBptsb3bVkopb8w8oy/fOyeVp9bs5dxBcZw/PMGn63e5DLsLj7NpvxXgWw6UklNcAUCQQ0jvF8014/ozNjWGs1Nj6NczzKfbb+RNoOcB/T2eJ7tfaxQFnAH82z29VCKwRERmG2N0fFylVIfw4KUj2LivhPve2MoNE9OICHESEeokIsRJeHCQ9TzESXiIk4iQoBOPg53NZkQqr67jiwNHv9n7/uLAUY7V1AMQGxnC2JQYrspI5uyUGEYn9yI8xPs5VdujzfHQRSQI2AVciBXkG4FrjTEtngskIv8GftxWmOt46EqpQMsuPM71T2/gcPmpHeoLcTrcQe8kyCnkllZhDDgEhvaJ4uzUGMamWHvfqb0jWp071RfaNWORMaZeRO4GlgFO4BljzHYReQjINMYs8W25SinlH4MTerD+gQupb3BRWddAVW0DlbUNVNbWezxuoKqu3rr3fK3Weq2m3sV3z+7P2JQYzuzfk6iwluc9tYNXfejGmKXA0iav/byVttPaX5ZSSvlPkNNBtNNBdAcKY1/QK0WVUqqL0EBXSqkuQgNdKaW6CA10pZTqIjTQlVKqi9BAV0qpLkIDXSmluggNdKWU6iLavPTfbxsWKQL2n+bb44BiH5bja1pf+2h97dfRa9T6Tl+qMSa+pQW2BXp7iEhma2MZdARaX/tofe3X0WvU+vxDu1yUUqqL0EBXSqkuorMG+iK7C2iD1tc+Wl/7dfQatT4/6JR96EoppZrrrHvoSimlmtBAV0qpLqJDB7qIzBSRnSKSLSL3t7A8VERedy/fICJpAaytv4h8KiJZIrJdRH7YQptpIlImIl+4by1OCuLHGveJyDb3tpvN9yeWR92f35ciMjaAtQ3z+Fy+EJFyEflRkzYB//xE5BkRKRSRrzxeixWRFSKy230f08p7b3C32S0iNwSotj+KyNfu/793RKRXK+896XfBzzX+QkTyPP4fL2nlvSf9efdjfa971LZPRL5o5b0B+QzbxRjTIW9Y093tAQYCIcBWIL1Jm7uAJ9yP5wGvB7C+vsBY9+MorHlXm9Y3DfiXjZ/hPiDuJMsvAT4EBDgH2GDj//VhrAsmbP38gCnAWOArj9f+ANzvfnw/8PsW3hcL5LjvY9yPYwJQ2wwgyP349y3V5s13wc81/gJrnuG2vgMn/Xn3V31Nlv8J+Lmdn2F7bh15D308kG2MyTHG1AKvAXOatJkDPO9+/CZwofhzdlYPxph8Y8xm9+NjwA4gKRDb9qE5wAvGsh7oJSJ9bajjQmCPMeZ0rxz2GWPMaqCkycue37PngctbeOvFwApjTIkxphRYAcz0d23GmOXGmHr30/VAsi+3eapa+fy84c3Pe7udrD53dlwNvOrr7QZKRw70JOCgx/NcmgfmN23cX+oyoHdAqvPg7uoZA2xoYfFEEdkqIh+KyMjAVoYBlovIJhFZ2MJybz7jQJhH6z9Edn5+jfoYY/Ldjw8DfVpo0xE+y5ux/uJqSVvfBX+7290t9EwrXVYd4fM7DygwxuxuZbndn2GbOnKgdwoi0gN4C/iRMaa8yeLNWN0IZwJ/A94NcHmTjTFjgVnA90VkSoC33yYRCQFmA2+0sNjuz68ZY/3t3eHO9RWRB4F64OVWmtj5XXgcGAScBeRjdWt0RPM5+d55h/956siBngf093ie7H6txTYiEgT0BI4EpDprm8FYYf6yMebtpsuNMeXGmOPux0uBYBGJC1R9xpg8930h8A7Wn7WevPmM/W0WsNkYU9B0gd2fn4eCxq4o931hC21s+yxF5EbgMuA69y+cZrz4LviNMabAGNNgjHEBT7WybVu/i+78uAJ4vbU2dn6G3urIgb4RGCIiA9x7cfOAJU3aLAEazya4CviktS+0r7n72/4J7DDG/LmVNomNffoiMh7r8w7ILxwRiRSRqMbHWAfPvmrSbAmwwH22yzlAmUfXQqC0uldk5+fXhOf37AbgvRbaLANmiEiMu0thhvs1vxKRmcBPgdnGmMpW2njzXfBnjZ7HZea2sm1vft796SLga2NMbksL7f4MvWb3UdmT3bDOwtiFdfT7QfdrD2F9eQHCsP5UzwY+BwYGsLbJWH96fwl84b5dAtwB3OFuczewHeuI/Xrg3ADWN9C93a3uGho/P8/6BHjM/fluAzIC/P8biRXQPT1es/Xzw/rlkg/UYfXj3oJ1XGYlsBv4GIh1t80AnvZ4783u72I2cFOAasvG6ntu/A42nvXVD1h6su9CAD+/F93fry+xQrpv0xrdz5v9vAeiPvfrzzV+7zza2vIZtueml/4rpVQX0ZG7XJRSSp0CDXSllOoiNNCVUqqL0EBXSqkuQgNdKaW6CA10pZTqIjTQlVKqi/j/pOblxxtLhWUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_vec)\n",
    "plt.plot(eloss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example \n",
    "model = BertPromptForMultipleChoice.from_pretrained(base_model, config)\n",
    "model.to('cuda')\n",
    "loss_vec = []\n",
    "for i in range(20):# epochs = 20 \n",
    "    for batch in train_loader:  \n",
    "        batch = {k: batch[k].to('cuda') for k in batch}\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vec.append(loss.item())\n",
    "    print(loss_vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9324690",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gradient-clipping'''\n",
    "# if hasattr(optimizer, \"clip_grad_norm\"):\n",
    "#     # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n",
    "#     optimizer.clip_grad_norm(max_grad_norm)\n",
    "# elif hasattr(model, \"clip_grad_norm_\"):\n",
    "#     # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n",
    "#     model.clip_grad_norm_(max_grad_norm)\n",
    "# else: # fall back to normal torch utils\n",
    "#     nn.utils.clip_grad_norm_(\n",
    "#         model.parameters(),\n",
    "#         max_grad_norm,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025291f",
   "metadata": {},
   "source": [
    "## To-add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
